# Enhanced Training Methods: GSPO and Dr. GRPO Integration

## Overview

This document describes the integration of advanced training methods into the Droid Fine-Tuning system, specifically **GSPO (Group Sparse Policy Optimization)** and **Dr. GRPO (Doctor GRPO)** based on MLX-LM-LORA v0.8.1 capabilities.

## üÜï New Training Methods

### 1. SFT (Supervised Fine-Tuning) - ‚≠ê‚≠ê
**The classic approach for general instruction following**

- **Use Case**: General instruction following and task adaptation
- **Data Format**: `instruction_response` or `chat_messages`
- **Resource Intensity**: Medium
- **Best For**: Standard fine-tuning tasks, general domain adaptation

### 2. GSPO (Group Sparse Policy Optimization) - ‚≠ê‚≠ê‚≠ê‚≠ê üÜï Most Efficient
**Latest breakthrough in efficient reasoning model training**

- **Use Case**: Efficient reasoning tasks with resource constraints
- **Data Format**: `reasoning_chains` with sparse optimization markers
- **Resource Intensity**: Medium (2x faster than GRPO)
- **Estimated Speedup**: 2x faster than GRPO
- **Best For**: Resource-constrained environments requiring reasoning capabilities

**Key Parameters:**
- `sparse_ratio` (0.1-0.9): Fraction of reasoning steps to optimize
- `efficiency_threshold` (0.5-1.0): Minimum efficiency score to maintain
- `sparse_optimization` (boolean): Enable sparse attention patterns

### 3. Dr. GRPO (Doctor GRPO) - ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê üÜï Domain Expert
**Domain-specialized reasoning for expert knowledge applications**

- **Use Case**: Medical, scientific, and specialized domain reasoning
- **Data Format**: `domain_reasoning_chains` with domain context
- **Resource Intensity**: High
- **Best For**: Professional domain applications requiring expert-level reasoning

**Key Parameters:**
- `domain`: general, medical, scientific, legal, technical
- `expertise_level`: beginner, intermediate, advanced, expert
- `domain_adaptation_strength` (0.1-2.0): Strength of domain-specific adaptation

### 4. GRPO (Group Relative Policy Optimization) - ‚≠ê‚≠ê‚≠ê‚≠ê
**DeepSeek-R1 style multi-step reasoning capabilities**

- **Use Case**: Complex multi-step reasoning and problem solving
- **Data Format**: `reasoning_chains`
- **Resource Intensity**: High
- **Best For**: Complex reasoning tasks, mathematical problem solving

**Key Parameters:**
- `reasoning_steps` (3-15): Number of reasoning steps to train
- `multi_step_training` (boolean): Train on intermediate reasoning steps

## üèóÔ∏è Architecture Overview

The enhanced training system is built with a modular architecture:

```
backend/
‚îú‚îÄ‚îÄ training_methods.py      # Core method configurations and validation
‚îú‚îÄ‚îÄ main_enhancements.py     # Enhanced training manager integration
‚îî‚îÄ‚îÄ main.py                  # Modified with enhanced API endpoints

frontend/src/
‚îú‚îÄ‚îÄ types/enhancedTraining.ts    # TypeScript definitions
‚îú‚îÄ‚îÄ pages/EnhancedSetupPage.tsx  # Method selection UI
‚îî‚îÄ‚îÄ styles/enhanced-setup.css    # Enhanced styling
```

## üîÑ API Endpoints

### Enhanced Training Endpoints

- `GET /api/training/methods` - Get available training methods
- `POST /api/training/validate-data` - Validate training data format
- `POST /api/training/estimate-resources` - Estimate resource requirements
- `POST /api/training/start-enhanced` - Start enhanced training
- `POST /api/training/generate-sample-data` - Generate sample training data

### Example API Calls

```javascript
// Get available methods
const response = await fetch('/api/training/methods');
const { methods } = await response.json();

// Start GSPO training
const config = {
  training_method: 'gspo',
  model_path: '/path/to/model',
  train_data_path: '/path/to/data.jsonl',
  sparse_ratio: 0.7,
  efficiency_threshold: 0.85,
  sparse_optimization: true,
  // ... other parameters
};

const result = await fetch('/api/training/start-enhanced', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify(config)
});
```

## üìä Data Formats

### GSPO Data Format
```json
{
  "problem": "What is the most efficient way to solve X?",
  "reasoning_steps": [
    "Step 1: Identify key constraints",
    "Step 2: Apply optimization principles",
    "Step 3: Verify solution efficiency"
  ],
  "solution": "The optimal solution is...",
  "sparse_indicators": [1, 1, 0],
  "efficiency_markers": {
    "computation_cost": "low",
    "optimization_applied": true
  }
}
```

### Dr. GRPO Data Format
```json
{
  "problem": "Patient presents with symptoms X, Y, Z",
  "reasoning_steps": [
    "Gather patient history",
    "Perform physical examination",
    "Consider differential diagnoses",
    "Order appropriate diagnostic tests"
  ],
  "solution": "Diagnosis and treatment plan",
  "domain": "medical",
  "expertise_level": "advanced",
  "domain_context": {
    "specialty": "internal_medicine",
    "complexity": "high"
  }
}
```

### GRPO Data Format
```json
{
  "problem": "Complex reasoning problem",
  "reasoning_steps": [
    "Step 1: Problem analysis",
    "Step 2: Strategy formulation",
    "Step 3: Solution execution"
  ],
  "solution": "Final answer with reasoning"
}
```

## üöÄ Getting Started

### 1. Backend Setup

The enhanced training methods are automatically integrated when you run the application. No additional setup required.

### 2. Frontend Integration

```typescript
import { EnhancedSetupPage } from './pages/EnhancedSetupPage';
import { TrainingMethod } from './types/enhancedTraining';

// Use in your React app
<EnhancedSetupPage 
  onStartTraining={handleStartTraining}
  isTraining={isTraining}
/>
```

### 3. Method Selection

1. Open the Enhanced Setup Page
2. Select your preferred training method (SFT, GSPO, Dr. GRPO, or GRPO)
3. Configure method-specific parameters
4. Validate your data format
5. Review resource estimation
6. Start training

## üîç Resource Estimation

The system provides automatic resource estimation based on:

- Selected training method
- Model size (extracted from path)
- Dataset size
- Method-specific multipliers

**Example estimations:**
- **GSPO**: 1.2x memory, 0.5x time (most efficient)
- **Dr. GRPO**: 1.5x memory, 1.3x time (most capable)
- **GRPO**: 1.3x memory, 1.0x time (balanced)

## üß™ Testing and Validation

### Data Validation
The system automatically validates data formats for each method:

```python
# Validate GSPO data
validation = TrainingDataValidator.validate_data_format(
    TrainingMethod.GSPO, 
    "path/to/data.jsonl"
)
```

### Sample Data Generation
Generate sample data for testing:

```python
# Generate GSPO samples
enhanced_manager.generate_sample_data(
    "gspo", 
    "/tmp/sample_gspo_data.jsonl", 
    num_samples=20
)
```

### Running Tests
```bash
python test_integration.py
```

## üîß Configuration Parameters

### Base Configuration (All Methods)
- `model_path`: Path to the base model
- `train_data_path`: Path to training data
- `val_data_path`: Path to validation data (optional)
- `learning_rate`: Learning rate (1e-6 to 1e-4)
- `batch_size`: Batch size (1-8)
- `max_seq_length`: Maximum sequence length (512-8192)
- `iterations`: Number of training iterations
- `early_stop`: Enable early stopping
- `patience`: Early stopping patience

### GSPO Specific
- `sparse_ratio`: Fraction of reasoning steps to optimize (0.1-0.9)
- `efficiency_threshold`: Minimum efficiency score (0.5-1.0)
- `sparse_optimization`: Enable sparse attention patterns

### Dr. GRPO Specific
- `domain`: Target domain (general, medical, scientific, legal, technical)
- `expertise_level`: Target expertise (beginner, intermediate, advanced, expert)
- `domain_adaptation_strength`: Adaptation strength (0.1-2.0)

### GRPO Specific
- `reasoning_steps`: Number of reasoning steps (3-15)
- `multi_step_training`: Enable intermediate step training

## üéØ Best Practices

### Choosing the Right Method

1. **Use SFT** for general instruction following and standard fine-tuning
2. **Use GSPO** when you need reasoning capabilities but have resource constraints
3. **Use Dr. GRPO** for domain-specific applications requiring expert knowledge
4. **Use GRPO** for complex multi-step reasoning tasks

### Data Preparation

1. **Ensure data quality**: Use high-quality, domain-relevant training data
2. **Format validation**: Always validate data format before training
3. **Sample generation**: Use the built-in sample generators for testing
4. **Size considerations**: Start with smaller datasets for initial testing

### Resource Management

1. **Monitor memory usage**: Check resource estimation before training
2. **Optimize parameters**: Adjust batch size and sequence length based on available resources
3. **Use quantization**: Consider 4-bit quantization for large models
4. **Close other applications**: Free up memory during training

## üîÑ Backward Compatibility

All enhanced training methods are fully backward compatible with existing SFT functionality. Existing training configurations will continue to work without modification.

## üöß Future Enhancements

The architecture supports easy addition of new training methods from MLX-LM-LORA v0.8.1:

- **DPO (Direct Preference Optimization)**
- **CPO (Contrastive Preference Optimization)**
- **ORPO (Odds Ratio Preference Optimization)**
- **Online DPO**
- **XPO (Cross Preference Optimization)**
- **RLHF (Reinforcement Learning from Human Feedback)**

## üìû Support

For issues related to enhanced training methods:

1. Check the data format validation output
2. Review resource estimation recommendations
3. Ensure MLX environment is properly configured
4. Run integration tests to verify setup

## üéâ Summary

The enhanced training methods integration brings cutting-edge capabilities to the Droid Fine-Tuning system:

- **4 training methods** including latest GSPO and Dr. GRPO
- **Automatic data validation** and format conversion
- **Resource estimation** and optimization recommendations
- **Sample data generation** for testing
- **Beautiful UI** with method selection and configuration
- **Full backward compatibility** with existing functionality

Start exploring these advanced training methods to unlock new capabilities in your MLX fine-tuning workflows!